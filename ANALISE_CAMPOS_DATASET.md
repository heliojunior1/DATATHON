# üìä AN√ÅLISE DETALHADA DO DATASET PEDE 2024

## üîç Estrutura Geral
- **Total de registros**: ~1.349 alunos
- **Total de colunas**: 69 colunas
- **Anos cobertos**: 2020, 2021, 2022 (+ dados de 2024 no arquivo principal)
- **Formato**: Wide format (uma linha por aluno, colunas com sufixo _YYYY)

---

## 1Ô∏è‚É£ CAMPOS COMUNS ENTRE OS ANOS (2020, 2021, 2022)

### ‚úÖ Campos Num√©ricos Dispon√≠veis em Todos os Anos

| Campo | Descri√ß√£o | Dispon√≠vel em |
|-------|-----------|---------------|
| **INDE** | √çndice de Desenvolvimento Educacional (principal) | 2020, 2021, 2022 |
| **IAA** | √çndice de Auto-Avalia√ß√£o | 2020, 2021, 2022 |
| **IEG** | √çndice de Engajamento (li√ß√µes de casa) | 2020, 2021, 2022 |
| **IPS** | √çndice Psicossocial | 2020, 2021, 2022 |
| **IDA** | √çndice de Desempenho Acad√™mico | 2020, 2021, 2022 |
| **IPP** | √çndice Psicopedag√≥gico | 2020, 2021, 2022 |
| **IPV** | √çndice de Ponto de Virada | 2020, 2021, 2022 |
| **IAN** | √çndice de Adequa√ß√£o de N√≠vel | 2020, 2021, 2022 |

### ‚úÖ Campos Categ√≥ricos Dispon√≠veis em Todos os Anos

| Campo | Descri√ß√£o | Valores Poss√≠veis |
|-------|-----------|-------------------|
| **PEDRA** | Classifica√ß√£o do aluno | Ametista, √Ågata, Quartzo, Top√°zio |
| **PONTO_VIRADA** | Indica ponto de virada | Sim, N√£o |
| **FASE** | Fase/n√≠vel do aluno | 0, 1, 2, 3, 4, 5, etc. |
| **TURMA** | Turma do aluno | A, B, C, D, ... |

### ‚ö†Ô∏è Campos com Dados Parciais

| Campo | Anos Dispon√≠veis | Problema |
|-------|------------------|----------|
| **DEFASAGEM** | Apenas 2021, 2022 | **N√ÉO existe em 2020** |
| **NIVEL_IDEAL** | Apenas 2021, 2022 | N√ÉO existe em 2020 |
| **INSTITUICAO_ENSINO** | Todos os anos | Nomes ligeiramente diferentes |
| **NOTA_PORT/MAT/ING** | Apenas 2022 | **Muitos nulos (~80%)** |

---

## 2Ô∏è‚É£ AN√ÅLISE DE VALORES NULOS

### üìä Taxa de Nulos por Tipo de Dado

Com base na estrutura dos dados:

#### **BAIXA TAXA DE NULOS (< 30%)** ‚úÖ - USAR
- **INDE** (√≠ndices principais): ~20-25% nulos
- **IAA, IEG, IPS, IDA, IPP, IPV, IAN**: ~20-30% nulos
- **PEDRA**: ~20-25% nulos
- **FASE**: ~30% nulos
- **PONTO_VIRADA**: ~25% nulos

#### **M√âDIA TAXA DE NULOS (30-60%)** ‚ö†Ô∏è - USAR COM CAUTELA
- **DEFASAGEM_2021**: ~35-40% nulos
- **DEFASAGEM_2022**: ~25-30% nulos
- **INSTITUICAO_ENSINO**: ~25% nulos

#### **ALTA TAXA DE NULOS (> 60%)** ‚ùå - EVITAR
- **NOTA_PORT_2022, NOTA_MAT_2022, NOTA_ING_2022**: ~70-80% nulos
- **REC_EQUIPE_***: ~40-60% nulos
- **DESTAQUE_***: Textos livres, dif√≠cil processamento
- **CG_2022, CF_2022, CT_2022**: Apenas em 2022, ~25% nulos

### üéØ Por que Tantos Nulos?

1. **Alunos entraram em anos diferentes**:
   - Aluno que entrou em 2022 = TODOS os campos de 2020 e 2021 s√£o nulos
   - Exemplo: ALUNO-2 tem apenas dados de 2022

2. **Alunos que sa√≠ram/abandonaram**:
   - Aluno em 2020-2021 mas n√£o em 2022 = campos 2022 nulos
   - Exemplo: ALUNO-10 tem dados s√≥ de 2020

3. **Campos novos adicionados com o tempo**:
   - DEFASAGEM s√≥ existe a partir de 2021
   - Notas espec√≠ficas s√≥ em 2022

---

## 3Ô∏è‚É£ CAMPOS RECOMENDADOS PARA MACHINE LEARNING

### üéØ FEATURES PRINCIPAIS (usar definitivamente)

#### Features Num√©ricas - Requerem **Normaliza√ß√£o**

```python
NUMERICAL_FEATURES = [
    'INDE',      # √çndice geral - MAIS IMPORTANTE
    'IAA',       # Auto-avalia√ß√£o
    'IEG',       # Engajamento (li√ß√µes)
    'IPS',       # Psicossocial
    'IDA',       # Desempenho acad√™mico
    'IPP',       # Psicopedag√≥gico
    'IPV',       # Ponto de virada
    'IAN',       # Adequa√ß√£o de n√≠vel
]
```

**Normaliza√ß√£o recomendada**: `StandardScaler` (m√©dia=0, desvio=1)
- Motivo: √çndices j√° est√£o em escala 0-10, mas vari√¢ncias diferentes
- Alternativa: `MinMaxScaler` para manter em [0,1]

#### Features Categ√≥ricas - Requerem **Encoding**

```python
CATEGORICAL_FEATURES = [
    'PEDRA',           # 4 categorias: Ametista, √Ågata, Quartzo, Top√°zio
    'PONTO_VIRADA',    # 2 categorias: Sim, N√£o
]
```

**Encoding recomendado**:
- **PEDRA**: `OrdinalEncoder` com ordem: Top√°zio(0) < Quartzo(1) < √Ågata(2) < Ametista(3)
  - Porque existe uma hierarquia natural de desempenho
- **PONTO_VIRADA**: `LabelEncoder` ou simplesmente: Sim=1, N√£o=0

### üîÑ FEATURES DERIVADAS (criar a partir dos dados)

```python
ENGINEERED_FEATURES = [
    'DELTA_INDE',           # INDE_ano_atual - INDE_ano_anterior
    'DELTA_IDA',            # IDA_ano_atual - IDA_ano_anterior
    'DELTA_IEG',            # IEG_ano_atual - IEG_ano_anterior
    'MEDIA_INDICES',        # (IAA + IEG + IPS + IDA + IPP + IPV + IAN) / 7
    'ANOS_NO_PROGRAMA',     # Quantos anos o aluno est√° na Passos M√°gicos
    'MUDOU_PEDRA',          # 1 se PEDRA mudou entre anos, 0 caso contr√°rio
    'TEVE_PONTO_VIRADA',    # 1 se teve ponto virada alguma vez
]
```

### ‚ö†Ô∏è FEATURES A EVITAR

```python
AVOID_FEATURES = [
    'NOME',                          # Identificador, n√£o feature
    'DESTAQUE_IEG/IDA/IPV',         # Texto livre, dif√≠cil processar
    'REC_EQUIPE_*',                 # Muitos nulos, muitas categorias
    'REC_AVA_*',                    # Muitos nulos
    'NOTA_PORT/MAT/ING',            # 70-80% nulos
    'TURMA',                        # Identificador, n√£o informativo
    'INDE_CONCEITO',                # Redundante com INDE num√©rico
    'CG_2022, CF_2022, CT_2022',    # S√≥ em 2022, desconhecidos
]
```

---

## 4Ô∏è‚É£ TARGET VARIABLE (Vari√°vel Alvo)

### üéØ Objetivo: Prever Mudan√ßa na DEFASAGEM

**Problema**: DEFASAGEM s√≥ existe em 2021 e 2022, n√£o em 2020!

### Op√ß√£o 1: Regress√£o (Valor Cont√≠nuo)

```python
# Target: quanto a DEFASAGEM vai mudar
TARGET = DEFASAGEM_2022 - DEFASAGEM_2021
```

**Interpreta√ß√£o**:
- `TARGET < 0`: Aluno **melhorou** (defasagem diminuiu)
- `TARGET = 0`: Aluno **manteve** (sem mudan√ßa)
- `TARGET > 0`: Aluno **piorou** (defasagem aumentou)

**Modelos recomendados**:
- Linear Regression
- Ridge Regression
- Random Forest Regressor
- XGBoost Regressor

### Op√ß√£o 2: Classifica√ß√£o (3 Classes)

```python
# Target: categoria de mudan√ßa
def classify_change(delta):
    if delta < -0.5:
        return 'MELHOROU'      # -1
    elif delta > 0.5:
        return 'PIOROU'        # 1
    else:
        return 'MANTEVE'       # 0
```

**Modelos recomendados**:
- Logistic Regression
- Random Forest Classifier
- XGBoost Classifier
- SVM

### ‚ö†Ô∏è Desafio Importante

Como DEFASAGEM n√£o existe em 2020:
- **S√≥ podemos usar dados de 2021 para prever 2022**
- Ou criar DEFASAGEM_2020 manualmente: `FASE_2020 - NIVEL_IDEAL` (mas NIVEL_IDEAL tamb√©m n√£o existe em 2020!)

**Solu√ß√£o**:
1. **Focar em 2021 ‚Üí 2022**: Usar features de 2021 para prever DELTA_DEFASAGEM em 2022
2. **Criar proxy de DEFASAGEM_2020**: Usar FASE e IDADE para estimar defasagem em 2020

---

## 5Ô∏è‚É£ ESTRAT√âGIA DE LIMPEZA E PREPARA√á√ÉO

### üìã Pipeline Recomendado

```python
# 1. FILTRAR ALUNOS V√ÅLIDOS
# Manter apenas alunos com dados em 2+ anos consecutivos
valid_students = df[
    (df['INDE_2021'].notna()) & (df['INDE_2022'].notna())
]

# 2. SELECIONAR FEATURES
features_2021 = ['INDE_2021', 'IAA_2021', 'IEG_2021', 'IPS_2021',
                 'IDA_2021', 'IPP_2021', 'IPV_2021', 'IAN_2021',
                 'PEDRA_2021', 'PONTO_VIRADA_2021']

# 3. CRIAR TARGET
target = valid_students['DEFASAGEM_2022'] - valid_students['DEFASAGEM_2021']

# 4. IMPUTAR NULOS (para features num√©ricas)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
X_numeric = imputer.fit_transform(X_numeric)

# 5. NORMALIZAR FEATURES NUM√âRICAS
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_numeric_scaled = scaler.fit_transform(X_numeric)

# 6. ENCODAR FEATURES CATEG√ìRICAS
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder(categories=[
    ['Top√°zio', 'Quartzo', '√Ågata', 'Ametista']  # ordem de melhor desempenho
])
X_pedra_encoded = encoder.fit_transform(X_pedra)

# 7. REMOVER OUTLIERS (opcional)
from scipy import stats
z_scores = np.abs(stats.zscore(X_numeric_scaled))
X_clean = X_numeric_scaled[(z_scores < 3).all(axis=1)]
```

---

## 6Ô∏è‚É£ ESTRAT√âGIA DE TREINO/VALIDA√á√ÉO/TESTE

### üìä Divis√£o dos Dados

```python
from sklearn.model_selection import train_test_split

# Op√ß√£o 1: Split aleat√≥rio (80/10/10)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y_binned
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Op√ß√£o 2: Time-based split (mais realista)
# - Train: todos os dados 2021->2022
# - Validation: holdout de 20% dos alunos
# - Test: dados futuros 2022->2024 (quando dispon√≠veis)
```

### üéØ M√©tricas de Avalia√ß√£o

**Para Regress√£o**:
- MAE (Mean Absolute Error) - principal
- RMSE (Root Mean Squared Error)
- R¬≤ Score

**Para Classifica√ß√£o**:
- Accuracy
- F1-Score (macro)
- Confusion Matrix
- AUC-ROC

---

## 7Ô∏è‚É£ RESUMO EXECUTIVO - CAMPOS FINAIS

### ‚úÖ USAR ESTES CAMPOS (12 features)

| # | Feature | Tipo | Transforma√ß√£o | Prioridade |
|---|---------|------|---------------|------------|
| 1 | INDE | Num√©rico | StandardScaler | ‚≠ê‚≠ê‚≠ê ALTA |
| 2 | IAA | Num√©rico | StandardScaler | ‚≠ê‚≠ê M√âDIA |
| 3 | IEG | Num√©rico | StandardScaler | ‚≠ê‚≠ê‚≠ê ALTA |
| 4 | IPS | Num√©rico | StandardScaler | ‚≠ê‚≠ê M√âDIA |
| 5 | IDA | Num√©rico | StandardScaler | ‚≠ê‚≠ê‚≠ê ALTA |
| 6 | IPP | Num√©rico | StandardScaler | ‚≠ê‚≠ê M√âDIA |
| 7 | IPV | Num√©rico | StandardScaler | ‚≠ê‚≠ê M√âDIA |
| 8 | IAN | Num√©rico | StandardScaler | ‚≠ê‚≠ê‚≠ê ALTA |
| 9 | PEDRA | Categ√≥rico | OrdinalEncoder | ‚≠ê‚≠ê‚≠ê ALTA |
| 10 | PONTO_VIRADA | Categ√≥rico | LabelEncoder | ‚≠ê BAIXA |
| 11 | DELTA_INDE | Derivado | Criar: INDE_atual - INDE_anterior | ‚≠ê‚≠ê‚≠ê ALTA |
| 12 | DELTA_IDA | Derivado | Criar: IDA_atual - IDA_anterior | ‚≠ê‚≠ê M√âDIA |

### üéØ TARGET

```
TARGET = DEFASAGEM_2022 - DEFASAGEM_2021
```

---

## 8Ô∏è‚É£ PR√ìXIMOS PASSOS

1. ‚úÖ **Instalar Python e bibliotecas**
   ```bash
   pip install pandas numpy scikit-learn xgboost fastapi uvicorn
   ```

2. ‚úÖ **Executar script de an√°lise**
   ```bash
   python analyze_data.py
   ```

3. ‚úÖ **Criar pipeline de dados**
   - Implementar limpeza
   - Implementar transforma√ß√µes
   - Validar resultados

4. ‚úÖ **Treinar modelos baseline**
   - Linear Regression
   - Random Forest
   - XGBoost

5. ‚úÖ **Desenvolver API FastAPI**
   - Endpoints de predi√ß√£o
   - Endpoints de m√©tricas
   - Endpoints de retreinamento

---

## üìö REFER√äNCIAS T√âCNICAS

### Ranges dos √çndices (para normaliza√ß√£o)
- **INDE**: 0 a 10
- **IAA, IEG, IPS, IDA, IPP, IPV**: 0 a 10
- **IAN**: 0, 5, ou 10 (discreto)
- **DEFASAGEM**: -5 a +3 (aproximadamente)

### Distribui√ß√£o de PEDRA (2020)
- Ametista: 336 (46%)
- √Ågata: 171 (23%)
- Quartzo: 128 (18%)
- Top√°zio: 92 (13%)

**NOTA**: Dataset desbalanceado! Considerar:
- Stratified sampling
- Class weights
- SMOTE (para upsampling de classes minorit√°rias)
